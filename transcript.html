---
layout: page
title: "Transcript"
permalink: /transcript/
---

{% include sidebar.html %}  <!-- 사이드바 포함 -->

<div class="main-wrapper home-content" style="display: flex; align-items: flex-start;">

<!-- 왼쪽 컬럼: 소개 -->
<div class="intro-section" style="flex: 7;">

    <h1>Transcript</h1>
    <h2>1. Statistics</h2>

<table>
  <thead>
    <tr>
      <th>Course</th>
      <th>Grade</th>
      <th>Content</th>
      <th>Textbook</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Introduction to Statistics</td>
      <td>A+</td>
      <td>definition of probability, discrete and continuous random variable, sampling, estimation, hypothesis test, inference between two populations</td>
      <td>Korean textbook</td>
    </tr>
    <tr>
      <td>Statistical Methods</td>
      <td>A+</td>
      <td>One-way ANOVA, randomized complete block design, two-way ANOVA, chi-squared test, goodness-of-fit test, simple and multiple linear regression</td>      
      <td>instructor-prepared materials</td>
    </tr>  
    <tr>
      <td>R and Python Programming</td>
      <td>A+</td>
      <td>basic concepts of Python and R</td>      
      <td>instructor-prepared materials</td>
    </tr> 
    <tr>
      <td>Mathematical Statistics (1) (undergraduate course)</td>
      <td>A+</td>
      <td>probability set function, special expectations, multivariate dstribution, transformation of random variables, discrete random variables, continuous random variables, order statistics, consistency, delta-method, MGF, CLT</td>      
      <td>Hogg, <i>Introduction to Mathematical Statistics</i>, 8th ed.</td>
    </tr>
    <tr>
      <td>Mathematical Statistics II</td>
      <td>A+</td>
      <td>MLE, Rao–Cramér lower bound, maximum likelihood tests, MVUE, sufficient statistic, Lehmann–Scheffé theorem, exponential class, Neyman–Pearson theorem, UMP tests</td>
      <td>Hogg, <i>Introduction to Mathematical Statistics</i>, 8th ed.</td>
    </tr>
    <tr>
      <td>Regression Analysis</td>
      <td>A+</td>
      <td>inference and prediction in regression analysis, Bonferroni joint confidence intervals, Gauss-Markov theorem, model selection and validation, Cook’s distance, variance inflation factor, weighted least squares</td>
      <td>Kutner, <i>Applied Linear Regression Models</i>, 5th ed.</td>
    </tr>
    <tr>
      <td>Sampling Theory in Data Science</td>
      <td>A+</td>
      <td>simple random sampling, stratified random sampling, ratio estimation, regression estimation, cluster sampling, sampling with unequal probability, model-based approach</td>
      <td>Lohr, <i>Sampling: Design and Analysis</i>, 2nd ed.</td>
    </tr>  
    <tr>
      <td>Deep Learning</td>
      <td>A</td>
      <td>neural network, activation function, forward and back propagation, supervised and unsupervised learning, ROC curve, CNN, RNN, reinforcement learning, Bellman equation</td>
      <td>instructor-prepared materials</td>
    </tr>   
    <tr>
      <td>Statistical Computing</td>
      <td>A+</td>
      <td>core theory and algorithms in statistical computing: inverse transform method, rejection sampling, bootstrapping, variance reduction techniques, MCMC, Gibbs sampling, Newton–Raphson method</td>
      <td>Ross, <i>Simulation</i>, 5th ed.</td>
    </tr>
    <tr>
      <td>Time Series Analysis</td>
      <td>A+</td>
      <td>moving average method, exponential smoothing, decomposition, AR model, MA model, ARMA model, ARIMA model, SARIMA, transfer function, cross-correlation function, ARCH and GARCH model, VAR model</td>
      <td>instructor-prepared materials</td>
    </tr> 
    <tr>
      <td>Multivariate Analysis</td>
      <td>A+</td>
      <td>principal component analysis, factor analysis, maximum likelihood method, principal component method, cluster analysis, Fisher’s LDA.</td>
      <td>Wolfgang, <i>Applied Multivariate Statistical Analysis</i>, 4th ed.</td>
    </tr> 
    <tr>
      <td>Linear Model</td>
      <td>A+</td>
      <td>multivariate distributions, distribution of quadratic forms, estimability, generalized least squares estimator, nested hypotheses, ANOVA, contrast </td>
      <td>instructor-prepared materials</td>
    </tr>    
    <tr>
      <td>Bayesian Statistics</td>
      <td>A+</td>
      <td>posterior estimate, posterior predictive p-value, choice of prior distribution, implicit meaning of Bayesian analysis, Laplace approximation, variatonal linear regression, composition method</td>
      <td>Gelman, <i>Bayesian Data Analysis</i>, 3rd ed.</td>
    </tr>
    <tr>
      <td>Statistical Learning Theory</td>
      <td>A+</td>
      <td>canonical correlation analysis, Gauss Markov theorem, subset selection, linear methods for Classication, bases expansions, reproducing kernel Hilbert space, model assessment.</td>
      <td>Hastie, <i>The Elements of Statistical Learning</i>, 2nd ed.</td>
    </tr>
    <tr>
      <td>Statistical Theory for High-dimensional and Big Data</td>
      <td>A+</td>
      <td>Hoeding's inequality, Bernstein's inequality, metric entropy, covariance estimation in the operator norm, sparse PCA, Rademacher complexity</td>
      <td>instructor-prepared materials</td>
    </tr>   
    <tr>
      <td>Mathematical Statistics 1 (graduate course)</td>
      <td>A+</td>
      <td>advanced topics in mathematical statistics: Gumbel–Max trick, semi-supervised mean estimation, Efron–Stein inequality, Hoeffding’s inequality, median-of-means estimator, minimax estimation, Stein’s paradox</td>
      <td>Casella & Berger, <i>Statistical Inference</i>, 2nd ed.</td>
    </tr>
    <tr>
      <td>Advanced Bayesian Methods</td>
      <td>A+</td>
      <td>application to linear mixed model and GLM, Bayesian nonparametric regression, Goussian process, finite mixture models, Dirichlet process</td>
      <td>Gelman, <i>Bayesian Data Analysis</i>, 3rd ed.</td>
    </tr> 
    <tr>
      <td>Nonparametric Function Estimation</td>
      <td>A+</td>
      <td>kernel density estimator, bandwidth selection, local polynomial regression, B-spline, smoothing spline, reproducing kernel Hilbert spaces, smoothing spline ANOVA</td>
      <td>Wand, <i>Kernel Smoothing</i> & Chong Gu, <i>Smoothing Spline ANOVA Models</i>, 2nd ed.</td>
    </tr>
    <tr>
      <td>Monte Carlo Methods</td>
      <td>A+</td>
      <td>advanced MCMC and Monte Carlo methods: adaptive rejection sampling, Rao–Blackwellization, Pólya–Gamma augmentation, reversible jump MCMC, Hamiltonian Monte Carlo, approximate Bayesian computation, slice sampling, parallel tempering</td>
      <td>instructor-prepared materials</td>
    </tr>
    <tr>
      <td>Generalized Mixed Models</td>
      <td>A+</td>
      <td>hierarchical model, random effects, generalized linear models, linear mixed models, application to missing data, repeated measures ANOVA, generalized linear mixed model, repeated measures within repeated measures, generalized estimating equations</td>
      <td>McCulloch & Searle <i>Generalized, Linear, and Mixed Models</i></td>
    </tr>    
  </tbody>
</table>


    <h2>2. Mathematics</h2>

    <table>
      <thead>
        <tr>
          <th>Course</th>
          <th>Grade</th>
          <th>Content</th>
          <th>Textbook</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Engineering Mathematics(1)</td>
          <td>B+</td>
          <td>chain rule, mean value theorem, L'Hospital's rule, techniques of integration, polar coordinates, sequence and series</td>
          <td>Stewart, <i>Calculus: Early Transcendentals</i>, 6th ed.</td>
        </tr>
        <tr>
          <td>Engineering Mathematics(2)</td>
          <td>A+</td>
          <td>vector functions, partial derivatives, multiple integrals over regions and polar coordinates, Green's theorem, Stokes' theorem</td>
          <td>Stewart, <i>Calculus: Early Transcendentals</i>, 6th ed.</td>
        </tr>        
        <tr>
          <td>Engineering Mathematics(3)</td>
          <td>A+</td>
          <td>first-order ODE, second order ODE, nonhomogeneous ODE, systems of ODEs, Legendre’s equation, Frobenius method, Bessel’s equation, Laplace transform, convolution</td>
          <td>Kreyszig, <i>Advanced Engineering Mathematics</i>, 10th ed.</td>
        </tr>
        <tr>
          <td>Engineering Mathematics(4)</td>
          <td>A+</td>
          <td>gradient, divergence, curl of vector field, Fourier series, Fourier transform, heat equation, complex numbers, Cauchy–Riemann equations, trigonometric and hyperbolic functions</td>
          <td>Kreyszig, <i>Advanced Engineering Mathematics</i>, 10th ed.</td>
        </tr>
        <tr>
          <td>Probability and Random Variable</td>
          <td>A+</td>
          <td>An introductory probability course for undergraduate EE students, covering up to the scope of Mathematical Statistics(1) (excluding hypothesis testing).</td>      
          <td>Yates & Goodman, <i>Probability and Stochastic Processes</i>, 3rd ed.</td>
        </tr>
        <tr>
          <td>Linear Algebra and Its Application</td>
          <td>A+</td>
          <td>engineering students only, solution-focused: system of linear equations, matrix algebra, kernal and range, LU and QR decomposition, dimension theorem, diagonalization</td>
          <td>Anton, <i>Contemporary Linear Algebra</i></td>
        </tr>    
        <tr>
          <td>Analysis (1)</td>
          <td>A</td>
          <td>basic topology, sequences and series, uniform continuity, differentiation, Riemann–Stieltjes integral, uniform convergence.</td>
          <td>Rudin, <i>Principles of Mathematical Analysis</i>, 3rd ed.</td>
        </tr>
        <tr>
          <td>Linear Algebra</td>
          <td>A+</td>
          <td>An Applied Statistics Department course covering both theory (proofs) and computation; it shares the same topics as the engineering course.</td>
          <td>Anton, <i>Contemporary Linear Algebra</i></td>
        </tr> 
        <tr>
          <td>Real Analysis 1</td>
          <td>A+</td>
          <td>Lebesgue measure, measurable functions, Lebesgue integral, Fubini’s theorem, Lebesgue differentiation theorem, Hilbert spaces, Fatou’s theorem.</td>
          <td>Stein, <i>Real Analysis: Measure theory, Integration, and Hilbert spaces</i>.</td>
        </tr>        
      </tbody>
    </table>

    <h2>3. GPA</h2>
    <p>undergraduate : 4.02/4.3 <strong>(3.85/4.0)</strong></p>
    <p>master        : 4.3/4.3 <strong>(4.0/4.0)</strong></p>
    
<style>
  table {
    width: 100%;
    margin: 0;
    border-collapse: collapse;
    table-layout: fixed;
  }
  th, td {
    border: 1px solid #ccc;
    padding: 10px;
    text-align: center;
    word-break: break-word;   /*  긴 글 줄바꿈 */
    white-space: normal;      /* 공백을 기준으로 자동 줄바꿈 */
  }
  th {
    background-color: #f5f5f5;
    font-size: 15px;
  }
  td {
    font-size: 12px;
  }

  th:nth-child(1), td:nth-child(1) { width: 14%; }  /* Course */
  th:nth-child(2), td:nth-child(2) { width: 6%; }   /* Grade */
  th:nth-child(3), td:nth-child(3) { width: 55%; }  /* Content */
  th:nth-child(4), td:nth-child(4) { width: 25%; }  /* Textbook */
  
</style>

  </div>

  <!-- 오른쪽 컬럼: 프로필 -->

<div class="profile-section" style="flex: 0.5; text-align: center;">
   <!--  {% include author-profile.html %} -->
</div>


</div>
